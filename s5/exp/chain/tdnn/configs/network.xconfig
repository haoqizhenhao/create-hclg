
  input dim=40 name=input

  fixed-affine-layer name=lda input=Append(-2,-1,0,1,2) affine-transform-file=exp/chain/tdnn/configs/lda.mat

  # the first splicing is moved before the lda layer, so no splicing here
  relu-renorm-layer name=tdnn1 dim=1024
  relu-renorm-layer name=tdnn2 input=Append(-1,2) dim=1024
  relu-renorm-layer name=tdnn3 input=Append(-2,1) dim=1024
  relu-renorm-layer name=tdnn4 input=Append(-3,3) dim=1024
  relu-renorm-layer name=tdnn5 input=Append(-5,1) dim=1024

  ## adding the layers for chain branch
  relu-renorm-layer name=prefinal-chain input=tdnn5 dim=1024 target-rms=0.5
  output-layer name=output include-log-softmax=false dim=6508 max-change=1.5

  # adding the layers for xent branch
  # This block prints the configs for a separate output that will be
  # trained with a cross-entropy objective in the 'chain' models... this
  # has the effect of regularizing the hidden parts of the model.  we use
  # 0.5 / args.xent_regularize as the learning rate factor- the factor of
  # 0.5 / args.xent_regularize is suitable as it means the xent
  # final-layer learns at a rate independent of the regularization
  # constant; and the 0.5 was tuned so as to make the relative progress
  # similar in the xent and regular final layers.
  relu-renorm-layer name=prefinal-xent input=tdnn5 dim=1024 target-rms=0.5
  output-layer name=output-xent dim=6508 learning-rate-factor=5.0 max-change=1.5

